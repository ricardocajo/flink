from pyflink.common import Types, Row
from pyflink.datastream.connectors.kafka import KafkaSink, KafkaRecordSerializationSchema
from pyflink.datastream.formats.json import JsonRowSerializationSchema

# Define the schema for Kafka
row_type_info = Types.ROW_NAMED([
    'recordId', 'time', 'arrivalTime',
    'bid0Price', 'bid0Ccy', 'bid0Size', 'bid0DataSource', 'bid0AgeOffsetMsecs',
    'bid1Price', 'bid1Ccy', 'bid1Size', 'bid1DataSource', 'bid1AgeOffsetMsecs',
    'bid2Price', 'bid2Ccy', 'bid2Size', 'bid2DataSource', 'bid2AgeOffsetMsecs',
    'bid3Price', 'bid3Ccy', 'bid3Size', 'bid3DataSource', 'bid3AgeOffsetMsecs',
    'bid4Price', 'bid4Ccy', 'bid4Size', 'bid4DataSource', 'bid4AgeOffsetMsecs',
    'ask0Price', 'ask0Ccy', 'ask0Size', 'ask0DataSource', 'ask0AgeOffsetMsecs',
    'ask1Price', 'ask1Ccy', 'ask1Size', 'ask1DataSource', 'ask1AgeOffsetMsecs',
    'ask2Price', 'ask2Ccy', 'ask2Size', 'ask2DataSource', 'ask2AgeOffsetMsecs',
    'ask3Price', 'ask3Ccy', 'ask3Size', 'ask3DataSource', 'ask3AgeOffsetMsecs',
    'ask4Price', 'ask4Ccy', 'ask4Size', 'ask4DataSource', 'ask4AgeOffsetMsecs'
], [
    Types.INT(), Types.BIG_INT(), Types.BIG_INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT(),
    Types.DOUBLE(), Types.STRING(), Types.INT(), Types.INT(), Types.INT()
])


# Conversion between Table and DataStream
#ds = t_env.to_append_stream(t_env.from_path('raw_data_table'), row_type_info)

#ds_transformed = ds#.map(
#    lambda value: Row(
#        value[0],
#        value[1],
#        value[2],
#        "JOB 5 " + value[3].upper() if value[3] is not None else "JOB 5",
#        value[4]
#    ),
#    output_type=row_type_info
#)
#json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

#sink = KafkaSink.builder() \
#    .set_bootstrap_servers("kafka-1:9092,kafka-2:9092,kafka-3:9092") \
#    .set_record_serializer(
#        KafkaRecordSerializationSchema.builder()
#            .set_topic("quote-data-enriched")
#            .set_value_serialization_schema(json_serialization_schema)
#            .build()
#    ) \
#    .build()

# Add KafkaSink as a sink to the environment
#ds_transformed.sink_to(sink)

# Execute the job
#env.execute()