apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-client-config-ha
  labels:
    app: flink
  namespace: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager-ha
    taskmanager.numberOfTaskSlots: 3
    rest.address: flink-jobmanager-ha
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    queryable-state.proxy.ports: 6125
    jobmanager.memory.process.size: 1600m
    taskmanager.memory.process.size: 1728m
    parallelism.default: 2  

    # Flink HA configurations
    kubernetes.cluster-id: flink-ha
    high-availability.type: kubernetes
    high-availability.storageDir: s3a://flink-ha/recovery
    restart-strategy.type: fixed-delay
    restart-strategy.fixed-delay.attempts: 10
    kubernetes.namespace: flink
    # state.backend.type: filesystem
    state.backend.type: rocksdb
    state.backend.rocksdb.localdir: /opt/flink/rocksdb-state
    state.backend.fs.hdfs.hadoopconf: s3a://minio-service:9000/
    state.backend.incremental: true
    state.checkpoints.dir: s3a://flink-state/flink/checkpoints
    state.savepoints.dir: s3a://flink-state/flink/savepoints
    fs.s3a.access.key: minio
    fs.s3a.secret.key: minio123 
    fs.s3a.endpoint: http://minio-service:9000
    fs.s3a.connection.timeout: 2000
    fs.s3a.connection.establish.timeout: 2000
    fs.s3a.connection.ssl.enabled: false
    execution.checkpointing.interval: 5000
    s3.path.style.access: true
    containerized.master.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    containerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS: "flink-s3-fs-hadoop-1.18.0.jar"
    rest.flamegraph.enabled: true
    metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
  log4j-cli.properties: |+
    # This affects logging for both user code and Flink
    rootLogger.level = WARN
    rootLogger.appenderRef.console.ref = ConsoleAppender
    rootLogger.appenderRef.rolling.ref = RollingFileAppender

    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO

    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = INFO
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = INFO
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = INFO
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = INFO

    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

    # Log all infos in the given rolling file
    appender.rolling.name = RollingFileAppender
    appender.rolling.type = RollingFile
    appender.rolling.append = false
    appender.rolling.fileName = ${sys:log.file}
    appender.rolling.filePattern = ${sys:log.file}.%i
    appender.rolling.layout.type = PatternLayout
    appender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    appender.rolling.policies.type = Policies
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size=100MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.max = 10

    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF    
  demo-kafka-consumer.sql: |+
    CREATE TABLE alerts (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_windows (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-windows',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );

    CREATE TABLE alerts_linux (
      alert_id INT,
      host STRING,
      status STRING,
      os_type STRING,
      ts TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'alerts-linux',
        'scan.startup.mode' = 'earliest-offset',
        'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9092',
        'format' = 'json'
    );
  job4.py: |+
    from pyflink.table.table_environment import StreamTableEnvironment
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.common import Types, Row
    from pyflink.datastream.connectors.kafka import KafkaSink, KafkaRecordSerializationSchema
    from pyflink.datastream.formats.json import JsonRowSerializationSchema

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)

    # Define the schema for Kafka
    row_type_info = Types.ROW_NAMED(['user_id', 'item_id', 'category_id', 'behavior', 'ts'],
                                    [Types.INT(), Types.INT(), Types.INT(), Types.STRING(), Types.STRING()])
    t_env.execute_sql("""
        CREATE TABLE my_table (
            user_id INT,
            item_id INT,
            category_id INT,
            behavior STRING,
            ts STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'source',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9095',
            'format' = 'json'
        );
    """)

    # Conversion between Table and DataStream
    ds = t_env.to_append_stream(t_env.from_path('my_table'), row_type_info)

    ds_transformed = ds.map(
        lambda value: Row(
            value[0],
            value[1],
            value[2],
            "JOB 4 " + value[3].upper() if value[3] is not None else "JOB 4",
            value[4]
        ),
        output_type=row_type_info
    )
    json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

    sink = KafkaSink.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9095") \
        .set_record_serializer(
            KafkaRecordSerializationSchema.builder()
                .set_topic("sink")
                .set_value_serialization_schema(json_serialization_schema)
                .build()
        ) \
        .build()

    # Add KafkaSink as a sink to the environment
    ds_transformed.sink_to(sink)

    # Execute the job
    env.execute()
  job5.py: |+
    from pyflink.table.table_environment import StreamTableEnvironment
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.common import Types, Row
    from pyflink.datastream.connectors.kafka import KafkaSink, KafkaRecordSerializationSchema
    from pyflink.datastream.formats.json import JsonRowSerializationSchema

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)

    # Define the schema for Kafka
    row_type_info = Types.ROW_NAMED(['user_id', 'item_id', 'category_id', 'behavior', 'ts'],
                                    [Types.INT(), Types.INT(), Types.INT(), Types.STRING(), Types.STRING()])
    t_env.execute_sql("""
        CREATE TABLE my_table (
            user_id INT,
            item_id INT,
            category_id INT,
            behavior STRING,
            ts STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'source',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9095',
            'format' = 'json',
            'json.ignore-parse-errors' = 'true'
        );
    """)

    # Conversion between Table and DataStream
    ds = t_env.to_append_stream(t_env.from_path('my_table'), row_type_info)

    ds_transformed = ds.map(
        lambda value: Row(
            value[0],
            value[1],
            value[2],
            "JOB 5 " + value[3].upper() if value[3] is not None else "JOB 5",
            value[4]
        ),
        output_type=row_type_info
    )
    json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

    sink = KafkaSink.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9095") \
        .set_record_serializer(
            KafkaRecordSerializationSchema.builder()
                .set_topic("sink")
                .set_value_serialization_schema(json_serialization_schema)
                .build()
        ) \
        .build()

    # Add KafkaSink as a sink to the environment
    ds_transformed.sink_to(sink)

    # Execute the job
    env.execute()
  job6.py: |+
    from pyflink.table.table_environment import StreamTableEnvironment
    from pyflink.datastream import StreamExecutionEnvironment
    from pyflink.common import Types, Row
    from pyflink.datastream.connectors.kafka import KafkaSink, KafkaRecordSerializationSchema
    from pyflink.datastream.formats.json import JsonRowSerializationSchema

    # Create a StreamExecutionEnvironment
    env = StreamExecutionEnvironment.get_execution_environment()
    # Create a TableEnvironment
    t_env = StreamTableEnvironment.create(env)

    # Define the schema for Kafka
    row_type_info = Types.ROW_NAMED(['user_id', 'item_id', 'category_id', 'behavior', 'ts'],
                                    [Types.INT(), Types.INT(), Types.INT(), Types.STRING(), Types.STRING()])
    t_env.execute_sql("""
        CREATE TABLE my_table (
            user_id INT,
            item_id INT,
            category_id INT,
            behavior STRING,
            ts STRING
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'source',
            'scan.startup.mode' = 'earliest-offset',
            'properties.bootstrap.servers' = 'kafka-kafka-bootstrap:9095',
            'format' = 'json',
            'json.fail-on-missing-field' = 'true'
        );
    """)

    # Conversion between Table and DataStream
    ds = t_env.to_append_stream(t_env.from_path('my_table'), row_type_info)

    ds_transformed = ds.map(
        lambda value: Row(
            value[0],
            value[1],
            value[2],
            "JOB 6 " + value[3].upper() if value[3] is not None else "JOB 6",
            value[4]
        ),
        output_type=row_type_info
    )
    json_serialization_schema = JsonRowSerializationSchema.builder().with_type_info(row_type_info).build()

    sink = KafkaSink.builder() \
        .set_bootstrap_servers("kafka-kafka-bootstrap:9095") \
        .set_record_serializer(
            KafkaRecordSerializationSchema.builder()
                .set_topic("sink")
                .set_value_serialization_schema(json_serialization_schema)
                .build()
        ) \
        .build()

    # Add KafkaSink as a sink to the environment
    ds_transformed.sink_to(sink)

    # Execute the job
    env.execute()